# Kompresja JPEG 2000

## Intro

Jest ona realizowana za pomocą transformacji falkowej w standardowy sposób, tj. w standardowy sposób obraz jest dzielony na podpasma, które są kodowane niezależnie. W duzym skrocie podpasma to pomniejszone obrazy przetransformowane z wielokrotnym zastosowaniem filtrow dolno i/lub gornoprzepustowych, dla wyższych rzędów transformacji podpasma są pomniejszane wielokrotnie (generowane na podstawie jednego z podpasm nizszego rzedu). To chyba troche za krotki opis - dluzszy (po angielsku, na niecala strone) znajdzie Pan w moim artykule https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0168704 w podrozdziale "Lifting-based DWT in lossless JPEG 2000".
Z drugiej strony cz. 2 standardu JPEG 2000 zawiera rozszerzenia, ktore umozliwiają modyfikowanie transformacji falkowej - dzielenie obrazu na podpasma o innych kształtach oraz uycie (ale globalnie dla wszystkich podpasm danego obrazu) innej pary filtrów filtrów dolno i górnoprzepustowego. Mozliwe jest rowniez odwracalne przeksztalcenie histogramu obrazu, co znacznie poprawia współczynnik kompresji gdy histogram kompresowanego obrazu jest rzadki (pomiędzy często wykorzystywanymi poziomami jasnosci sa poziomy nieuzywane).  Wykorzystując te rozszerzenia można adaptacyjnie dobrać transformację dla danego obrazu tak aby poprawić uzyskiwany współczynnik kompresji, a obraz nadal będzie poprawnie dekodowany przez standardowy dekompresor.
Zadaniem w pracy dyplomowej byłoby opracowanie zaimplementowanie przebadanie i wybranie ostatecznej postaci heurystyki, która znajdzie transformacje odpowiednia dla danego obrazu. W pewnym zakresie heurystyka zastosuje metodę prób i błędów - w bedzie dokonywac kilku probnych podzialow obrazu na podpasma i sprawdzi ktory z podzialow jest najlepszy. Mozna dobierac podzial przyrostowo - na danym poziomie transformacji wyprobowac pare wariantow a następnie przejsc na wyzszy poziom (bedzie to dzialac dobrze, gdyz wazniejsze dla wyniku kompresji sa nizsze poziomy, poziom wyzszy ma pasma 4x mniejsze niz poziom bezposrednio nizszy i odpowiednio mniejszy wplyw na efekt kompresji calosci). Zarys heurystyki bym przygotowal. Heurystyka by dzialala niezaleznie od kompresora - wykonywala probne transformacje na obrazie i sprawdzala jak kompresuja sie podpasma (albo za pomoca faktycznego kompresora albo w przyblizeniu sprawdzajac entropie podpasm). Heurystyka moze wykorzystywac rownoleglosc na roznych poziomach. Sama transformacja podpasma moze byc wykonywana rownolegle (ma to sens dla wiekszych podpasm), rownolegle mozna rowniez probowac kilka wariantow podzialow na podpasma, a na poziomie calego obrazu rozne filtry. Heurystyke mozna zaimplementowac w C++ z wykorzystaniem jego nowych mozliwosci w tym wątków (to zadanie raczej na wątki niż procesy MPI) i zoptymalizowac dla procesora wielordzeniowego. Efektywny wariant wykorzystania rownoleglosci trzebaby wyznaczyc eksperymentalnie. Możnaby porównać wyniki heurystyki z wyczerpującym wyszukiwaniem transformacji. Praktycznym wynikiem pracy byłoby uzyteczne narzędzie (mogloby byc udostepnione publicznie) ktoro szybko wyznaczy transformacje dla danego obrazu umozliwiajac uzyskanie dla tego obrazu lepszego współczynnika kompresji JPEG 2000 - kosztem niewielkiego zwiększenia czasu kompresji (transformacja, nawet kilkukrotna jest duzo szybsza od kodowania podpasm).

## Description 

Opis ponizej jest przesadnie dlugi (miejscami rozwlekly - poniewaz czesciowo go dyktowalem), ale koncepcja jest calkiem prosta, i w znacznej czesci szczegolowo zaplanowana, najwiekszym zagadnieniem do rozwazenia i zaproponowania rozwiazania jest sposób zrownoleglenia implementacji. Mysle ze warto, żeby przed przeczytaniem przejrzal Pan strone Wikipedii o JPEG 2000 (po angielsku) https://en.wikipedia.org/wiki/JPEG_2000 oraz moj zalaczony artykul (https://doi.org/10.1007/s11042-019-08371-w) – ten który wspominalem w poprzednim mailu. Potem proszę przeczytac opis ponizej i potwierdzic ze odpowiada Panu propozycja takiego tematu (albo możemy porozmawiac o jego modyfikacji) – wtedy opracuje krotki (ponizej strony) opis tematu do systemu PD.

Realizacja techniczna 1-wymiarowej transformacji DWT opisana jest w zalaczonym artykule, w skrócie dekomponuje ona ciąg próbek na 2 podpasma – podpasmo H, którego próbki uzyskano przez filtrowanie górnoprzepustowe co 2-giej probki oryginalnego ciągu, oraz L otrzymane z filtrowania dolnoprzepustowego pozostałych próbek z oryginalnego ciągu próbek. W bezstratnym JPEG 2000 do filtrowania używana jest para prostych filtrów (wzory 3 i 4 w artykule), zazwyczaj filtrowanie realizowane jest „w miejscu” próbka po filtrowaniu nadpisuje oryginalną próbkę.

2-wymiarowa transformacja DWT rzędu 1 dekomponuje obraz na 4 podpasma najpierw stosując do wszystkich kolumn obrazu 1-wymiarowa transformacje DWT (powstają podpasma H i L) a następnie do wszystkich wierszy (jeszcze raz do już przetworzonych probek) stosujemy ta sama 1-wymiarowa transformacje DWT (otrzymujemy podpasma HL, LL, HH, LH). Transformacja wyzszego  rzędu (R+1) polega na zastosowaniu tej samej procedury (transformacji rzędu 1) do podpasma LL uzyskanego w wyniku transformacji bezpośrednio niższego rzędu (R). Wygenerowanie każdego kolejnego poziomu transformacji wymaga 4x mniej operacji.

Tak jak strescilem powyżej działa standardowy JPEG 2000 (bez rozszerzeń). Rozszerzenia części 2 standardu umożliwiają modyfikacje i dostosowanie transformacji do konkretnego obrazu. Dostępne są implementacje, np. popularny Kakadu https://kakadusoftware.com/ (darmowy do zast. niekomercyjnych), które uwzględniają wszystkie te rozszerzenia. Celem pracy będzie wykorzystanie wymienionych dalej modyfikacji transformacji DWT (i calego procesu kompresji). Ostatecznym wynikiem powinna być dopracowana heurystyka wyznaczająca indywidualnie dla danego obrazu transformację  wykorzystującą dostępne modyfikacje, heurystyka zaimplementowana w postaci zoptymalizowanej aplikacji wielowątkowej. Aplikacja po wyznaczeniu transformacji powinna wywołać zewnętrzny kompresor JPEG 2000 zgodny z rozszerzeniami części 2 (przymijmy, że będzie to Kakadu) tak, alby ten kompresor skompresował obraz przy pomocy wyznaczonej transformacji (wtedy dekompresja obrazu nie będzie wymagała dodatkowych operacji na obrazie – zatem mamy obraz skompresowany lepiej/silniej, ale nadal zgodnie ze standardem). Z góry trudno założyć, że wszystkie modyfikacje się sprawdzą, trzeba też wybrać szybki sposób decydowania czy i jak daną modyfikację stosować. Dlatego sądzę że najlepiej będzie jeżeli najpierw zostanie zaimplementowana bardziej ogólna wstępna heurystyka umożliwiająca sprawdzenie różnych wariantów (np. filtrów transformacji 1D, kryteriów wyboru wariantu transformacji). Ta implementacja może być już gotowa do zrównoleglenia, ale nie musi działać efektywnie. Następnie po zbadaniu które z wariantów  sprawdzają się w praktyce, można będzie pozostałe usunąć lub wyłączyć z heurystyki finalnej (albo zdefiniować warianty domyślne pozostawiając użytkownikowi mozliwość wyboru pozostałych) oraz zoptymalizować implementację. W przypadku dobrych wynikow można będzie (i warto będzie) udostepnic ja publicznie (np. Git), istnieja również calkiem powazne i wysoko punktowane czasopisma w których można opublikowac opis takiego softu.

3 modyfikacje transformacji które warto wypróbować:

Modyfikacja 1) dekompozycja obrazu na podpasma - de facto jedyna skomplikowana koncepcyjnie modyfikacja, choc propunję zastosować bardzo szybką jednoprzebiegową żarłoczną heurystykę.

Działanie heurystyki: analizujemy rzędy transformacji od 1-go do najwyższego (przyjmijmy, że to będzie 5, to powinien być parametr implementacji), dla danego rzędu sprawdzamy opcje:
A.      stosujemy transformację 1D do kolumn i do wierszy (normalna transformacja 2D DWT, powstają 4 podpasma, a po drodze przy okazji powstają 2 podpasma które są gotowym wynikiem opcji C poniżej),
B.      stosujemy 1D DWT tylko do wierszy (powstają 2 podpasma L i H, na potrzeby dalszego przetwarzania pasmo L traktujemy jako LL)
C.      stosujemy 1D DWT tylko do kolumn (powstają 2 podpasma L i H, na potrzeby dalszego przetwarzania pasmo L traktujemy jako LL)
D.      w ogóle nie stosujemy transformacji i rezygnujemy z wyższych rzędów transformacji 2D

i wybieramy najlepszą, po czym przechodzimy do wyższego rzędu aż nie osiągniemy rzędu maksymalnego (o ile nie wybrano wariantu D z powyższych, wtedy kończymy wcześniej). Przy czym jest kilka sposobów wyboru najlepszej opcji i można różnie interpretować rząd transformacji.
Można wyznaczyć entropię bezpamięciową podpasm (szybkla operacja, wzór na entropię w zalaczonym artykule w ostatnim akapicie podrozdzialu „experimental procedure” na s. 8, przy czym zakladamy ze 0*log(0)=0), zazwyczaj entropia wystarczająco dobrze estymuje efekty rzeczywistej kompresji. Nie musi tego robić idealnie, wystarczy by wyniki były proporcjonalne do współczynników kompresji, bo użyjemy jej nie do porównania z rzeczywistym współczynnikiem, tylko do wyboru między A, B, C i D.
Jednak entropia może nie sprawdzać się dobrze do porównywania podpasm o silnie odmiennej charakterystyce (faktyczny koder entropijny w JPEG 2000 jest kontekstowy, a my używamy entropii bezpamięciowej, czyli bezkontekstowej). Podpasmo LL podlegało tylko filtracji dolnoprzepustowej (i zazwyczaj będzie dalej transformowane przez wyższe rzędy transformaji), pozostałe – górnoprzepustowej oraz nie są dalej transformowane. Filtracja górnoprzepustowa niweluje wpływ kontekstu, wiec żeby osiągnąć podobny efekt dla filtracji dolnorzepustowej można zastosować podobną operację – entropie wyznaczać dla podpasma LL nie bezpośrednio, ale po zastosowaniu do niego predykcji (zamiast próbek podpasma liczymy entropie dla różnic pomiędzy daną próbką a jej wartością przewidywaną/predykowaną/ na podstawie jej sąsiadów). Ponieważ podpasmo LL będzie dalej transformowane przez kolejne rzędy transformacji, a my chcemy z pominięciem tych rzędów oszacować jak się skompresuje, to trzeba przetestować kilka metod predykcji i wybrać najlepszą, proponuję 3 następujące (oraz dodatkowo wyznaczanie entropii bez predykcji, aby było widac czy stosowanie predykcji jest uzasadnione): zastosowac taki filtr górnoprzepustowy, jak dla podpasm H (np. w poziomie) ale do wszytkich próbek podpasma LL, wyznaczac różnice pomiędzy danym pikselem a jego sąsiadem położonym na lewo od niego,  wyznaczac różnice pomiędzy danym pikselem a klasycznym predyktorem MED (mediana z 3 wartości: sąsiad_na_lewo, sąsiad_górny, sąsiad_na_lewo+sąsiad_górny- sąsiad_lewo_górna).
Można zasymulować działanie JPEG 2000 za pomocą zewnętrznie wywołanego JPEG 2000 (badawczo – żeby sprawdzić jak działa estymacja entropijna), czyli zapisać otrzymane podpasma jako obrazy i skompresować je niezmodyfikowanym JPEG 2000 z transformacją rzędu X, X=0 dla podpasm innych niż LL (te podpasma nie będą dalej transformowane), a dla LL użyć rząd wynoszący tyle ile pozostało rzędów transformacji do wykonania, np. jeżeli LL jest uzyskane po transformacji rzędu 2 to zostają 3 rzędy do 5 więc uzywamy JPEG 2000 z rzędem 3

Te sposoby by trzeba porównać a w finalnej wersji zostawić jako domyślny najlepszy sposób estymacji oraz jako opcję wykorzystanie rzeczywistego zewnetrznego kodera JPEG 2000 (potrzebny jeżeli ktoś będzie chciał bardziej poprawić współczynnik kompresji godząc się na dużo wiekszy koszt czasowy)

Rząd transformacji generowanej przez heurystykę jest z góry ograniczony (to powinien być parametr implementacji), jest to rząd maksymalny, bo jak heurystyka wybierze opcję D na niższym poziomie to na nim zakończy transformację (nie ma sensu generowanie kilku pustych poziomów D). Jednak można rząd maksymalny traktować trochę bardziej elastycznie – generować transformację której liczba operacji będzie mniej-więcej odpowiadać danemu rzędowi niezmodyfikowanego JPEG 2000. Opcje B i C wymagają połowy liczby operacji wykonywanej na pojedynczy rząd standardowej transformacji DWT (czyli przez opcję A). Można zatem jako warunek stopu heurystyki użyć wirtualnego rzędu, który będzie liczony tak, że każda wybrana opcja A liczy się jako 1, a opcja B lub C jako 0.5.

Typowa implementacja transformacji DWT działa „w miejscu” nie wymagajac dodatkowej pamięci, jednak skoro chodzi nam o prędkość to lepiej będzie nowe podpasma generować w nowym miejscu pamięci. Tym bardziej, że skoro nie nadpisujemy transformowanych danych, to źródłowe dane będą się nadawały do jednoczesnego wykorzystania jako tylko do odczytu przez wiele wątków.

Modyfikacja 2) zastosowanie innej pary filtrów (dolno i górnoprzepustowego). Wadą filtrów jest to, że są niedoskonałe, np. standardowy filtr dolnoprzepustowy propaguje szum wysokiej częstotliwości do filtrowanych próbek z pozostałych próbek (choć jako filtr dolnoprzepustowy nie powinien tego robić), górnoprzepustowy ma taką samą wadę. Dlatego dla mocno zaszumionych obrazów korzystne może być (i często jest) zastosowanie pustego filtra dolnoprzepustowego (takiego, który de facto nie modyfikuje próbek). To na pewno warto sprawdzać, z moich eksperymentów wynika, że można w ten sposób poprawić współczynniki kompresji obrazów nie-fotograficznych (np. zrzuty ekranu). Być może efektywne będą również inne filtry, np. 2 pary filtrów wspomniane w dokumentacji do Kakadu (filtr Haara oraz filtr 13x7), lub ich odmiany w których filtr dolnoprzepustowy jest zastąpiony filtrem pustym – można je uwzględnić w heurystyce wstępnej. Filtry są zadawane dla całego obrazu, więc sprawdzanie wielu w finalnej heurystyce byłoby kosztowne – trzeba będzie się zdecydować na 2 lub 3 pary filtrow.

Wynik modyfikacji 1 i 2 (2 bez filtrow Haara i 13x7) to tak na prawde przypadek szczegolny transformacji DWT z pomijaniem krokow o której jest zalaczony artykul, ale z inna heurystyka (i w art. jest uzyty zmodyfikowany JPEG 2000 bez rozszerzen) – będzie można się porownac do wynikow z artykulu.

Modyfikacja 3) zastosowanie pakowania histogramów, o którym już pisałem w poprzednim mailu – sprawdza się prawie zawsze (o ile histogram jest rzadki) – za pomocą heurystyki wstępnej można to zweryfikować, i prawdopodobnie zostawić tą operację jako domyslnie wlaczona w heurystyce finalnej dla obrazu (po sprawdzeniu czy obraz ma rzadki histogram). Jeszcze sprawdze jak to zrealizowac w Kakadu, aby operacja odwrotna nie wymagała niczego poza uzyciem standardowego dekompresora (nie widze odpowiednich opcji, może tu jest nazywane inaczej niż w standardzie, tam jest okreslane jako LUT-style non-linerar point transform/piece-wise linear function).

Ponadto trzeba będzie sprawdzic w jaki sposób postepowac z obrazami kolorowym. Obraz taki sklada się z 3 skladowych R G i B (przed kompresją obraz RGB jest transformowany do skladowej jasnosci i 2 skladowych barwy za pomoca bardzo prostej odwracalnej transformacji RCT – opisanej na stronie wiki o JPEG 2000 https://en.wikipedia.org/wiki/JPEG_2000  jako „Color components transformation”). Dla każdej składowej po transf. RCT można dobrać inną dekompozycję na podpasma (filtry muszą być takie same dla wszystkich składowych, histogramy prawdopodobnie można pakować dla każdej składowej niezależnie – nie weryfikowalem tego), ale być może prawie taki sam efekt da wyznaczenie dekompozycji dla skladowej jasnosci, a do pozostalych zastosowanie tej samej dekompozycji – to trzeba sprawdzic za pomoca heurystyki wstepnej.

Tak, jak pisalem heurystyka moze wykorzystywac rownoleglosc na roznych poziomach. Sama transformacja podpasma moze byc wykonywana rownolegle (ma to sens dla wiekszych podpasm), rownolegle mozna rowniez testowac 4 warianty podzialu na podpasma (a wlasciwie 2, bo D to wariant „nic nie rob”, a C powstaje przy okazji realizacji A), szacować kompresowalność wariantów (tu już 4 wariantow), a na poziomie calego obrazu np. stosowac różne filtry. Ostateczna decyzje trzeba będzie podjac po okresleniu heurystyki finalnej.

Najwazniejszym elementem pracy magisterskiej sa badania, tu jest bardzo duze pole do badan. Wyniki heurystyki finalnej można analizowac na wiele sposobow – wpływ jej zastosowania na współczynnik kompresji można badać porównując współczynnik kompresji do współczynnika uzyskanego z użyciem niezmodyfikowanego JPEG 2000 bez rozszerzen. Można również zastosować wyczerpujące poszukiwanie optymalnej transformacji i do niej też się porównać. Domyślnie implementacje JPEG 2000 stosują transformację rzędu 5. Nasza heurystyka sprawdza 4 warianty dekompozycji dla każdego rzędu transformacji, zakładając ten sam rząd transformacji mamy 4^5 mozliwości (=1024) do wypróbowania, to jeszcze trzeba pomnożyć przez liczbę sprawdzonych par filtrów (załóżmy 6), dodatkowo przez 2 (z lub bez pakowania histogramów), co razem daje 12 tys. kombinacji – do ogarnięcia przez komputer dla przeprowadzenia badań, choć zbyt kosztowne by tak robić w praktyce. Wreszcie można porównac uzyskane wyniki do wynikow heurystyki w podanym wczesniej artykule (a mają szanse być lepsze, bo tam wykorzystywano JPEG 2000 bez rozszerzen, w którym kodowanie podpasm było nie dostosowane do faktycznej dekompozycji na podpasma). Prędkość heurystyki trzeba odnieść do prędkości kompresji JPEG 2000 i ocenić czy poprawa współczynnika jest warta pogorszenia czasu kompresji (przy czym można porównać się z Kakadu, które prawdopodobnie jest optymalizowane na poziomie assemblera, oraz do jakiejś implementacji dostępnej ze źródłem, przy czym nie musi ona obsługiwać rozszerzeń standardu). Trzeba tez też przeanalizować przyspieszenie uzyskane przy wykorzystaniu różnych liczb wątków.
