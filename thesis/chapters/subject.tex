\section{Solution to the problem}

\subsection{Proposed algorithm}

Taking into consideration existing solutions mentioned in \fullref{ch:problem} it was decided to
implement several extensions of the JPEG 2000 standard, Part 2. Moreover, proposed algorithm
was designed to be parallelized. Such decision improves runtime performance especially in applications
that require processing of great number of images. As it was announced in \fullref{ch:intro}, the standard
way of computing discrete wavelet transform in the Part 1 of the JPEG 2000 standard is to decompose the
image into sub-bands using a pair of low- and high-pass filters. This decomposition is performed both in
vertical and horizontal directions. Moreover, it is applied multiple times across whole processing tree.

Vast majority of available extensions was mentioned in \fullref{sec:part2_jpeg2000}. It was decided to choose
solutions described in \fullref{sec:arbitrary_decomposition} and \fullref{sec:arbitrary_wavelet_transform} in the
planning phase of thesis development. As it was shown in the mentioned beforehand chapters, arbitrary decomposition
extension creates a possibility of splitting the image into sub-bands of different shapes. The latter feature,
i.e. arbitrary wavelet transform breaks strict selection rule of filters pair. With the help of these
extensions and carefully chosen JPEG 2000 coder estimation process it is possible to adaptively choose
best possible variant for the given image.

The workflow of the presented solution is as follows.

\begin{itemize}
    \item The image is chosen by the user in the command line interface.
    \item The image is transformed to grayscale or left in RGB space upon choice of the user.
    \item The scheduler determines number of present hardware threads able to run program.
    \item Each thread dispatches possible combination of decomposition from precomputed look up table.
    \item The chain of selected DWTs is calculated in the context of each thread.
    \item During decomposition process entropy of image is calculated and stored for later usage.
    \item Minimal entropy is chosen after finish of the above mentioned calculations.
    \item Process is repeated for other filter pairs.
    \item Global minimum entropy is chosen and the best combination is shown to the user.
\end{itemize}

\subsection{Different variants of DWT decomposition} \label{sec:dwt_decompositions}

As it was described in \fullref{sec:arbitrary_wavelet_transform}, the Part 2 of JPEG 2000 standard allows to perform
multiple modifications of decomposition even in the same step of DWT. The solution mentioned in \fullref{sec:kakadu} supports
such extensive set of DWT variants. However, in this solution the combinations were narrowed to the most
generic case. Moreover, the depth of discrete wavelet transform chain was limited to 5.

The chosen heuristic consists of four possible operation which are applied on the certain DWT level.
The first one is a transformation compliant with Part 1 of the JPEG 2000 standard, i.e. 2D DWT. Therefore, in a result
of such operation four sub-bands are created. The sub-band which was two time filtered with low-pass filter
is once again transformed in the next DWT level. Another two options imply on transforming given image only
row-wise or column-wise by applying 1D DWT. The low-pass filtered image is once again promoted to next DWT operations.
Last possible transformation is basically no operation and resignation from another calculations

\subsection{Selected filters}

Although the design of the application is flexible in terms of selecting the pair of filters, it was decided
to limit initial research to three options. The first option is Part 1 compliant solution, i.e. 5/3 LeGall filter.
The other two filters are Haar (due to being special case of the Daubechies wavelet, the Haar wavelet is also
known as ``db1'') and 13x7 biorthogonal wavelet. The first pair of filters is depicted in the Table \ref{tab:anal_synth_haar},
while the second one is presented in the Table \ref{tab:anal_synth_137}).

\begin{table}[!htb]
    \centering
    \caption{Analysis and synthesis filter taps for the Haar filter bank}
    \label{tab:anal_synth_haar}
\begin{tabular}{ccc}
    \toprule
    n         & Low-pass, $h_{0}(n)$ & Low-pass, $g_{0}(n)$ \\
    \midrule
    $0$       & 0.7071067811865476  & 0.7071067811865476  \\
    $1$       & 0.7071067811865476  & 0.7071067811865476  \\
    \bottomrule
\end{tabular}

\bigskip
\bigskip

\begin{tabular}{ccc}
    \toprule
    n         & High-pass, $h_{1}(n)$ & High-pass, $g_{1}(n)$ \\
    \midrule
    $0$       & -0.7071067811865476  & 0.7071067811865476  \\
    $1$       & 0.7071067811865476  & -0.7071067811865476  \\
    \bottomrule
\end{tabular}
\end{table}

\begin{table}[!htb]
    \centering
    \caption{Analysis and synthesis filter taps for the 13x7 biorthogonal wavelet filter bank}
    \label{tab:anal_synth_137}
\begin{tabular}{ccc}
    \toprule
    n         & Low-pass, $h_{0}(n)$ & Low-pass, $g_{0}(n)$ \\
    \midrule
    $0$       & +0.966747552403483     & +0.7071067811865476  \\
    $\pm 1$   & +0.4474660099696121    & +0.3535533905932738  \\
    $\pm 2$   & -0.16987135563661201   & -0.057543526228500  \\
    $\pm 3$   & -0.1077232986963881    & -0.091271763114250  \\
    $\pm 4$   & +0.04695630968816917   &                     \\
    $\pm 5$   & +0.013810679320049757  &                     \\
    $\pm 6$   & -0.006905339660024878  &                     \\
    \bottomrule
\end{tabular}

\bigskip
\bigskip


\begin{tabular}{ccc}
    \toprule
    n         & High-pass, $h_{1}(n)$ & High-pass, $g_{1}(n)$ \\
    \midrule
    $0$       & -0.7071067811865476    & -0.966747552403483  \\
    $\pm 1$   & +0.3535533905932738    & +0.4474660099696121  \\
    $\pm 2$   & -0.057543526228500     & +0.16987135563661201  \\
    $\pm 3$   & +0.091271763114250     & -0.1077232986963881  \\
    $\pm 4$   &                        & -0.04695630968816917 \\
    $\pm 5$   &                        & +0.013810679320049757 \\
    $\pm 6$   &                        & +0.006905339660024878 \\
    \bottomrule
\end{tabular}
\end{table}

\subsection{Entropy as JPEG 2000 coder estimator}

The entropy can be understood as the average level of ``information'' in the variable's possible outcomes. Other ways of
specifying this concept involve mentioning terms like ``surprise'' or ``uncertainty''. Another name that is linked to entropy comes from the paper
``A Mathematical Theory of Communication'' authored by Claude Shannon. This name is related to original author's surname as
this type of entropy is introduced in some sources as Shannon's entropy \cite{entropy_wiki}. The entropy formal
definition is as follows. A random discrete variable $X$ is given. Possible outcomes of this variable are defined as
$x_{1},\dots,x_{n}$. Then, the probability of occurrence of these outcomes is depicted as $P(x_{1}),\dots,P(x_{n})$.
With this set of variables the entropy of $X$ can be presented in the way as shown in the formula \ref{eq:entropy} \cite{entropy}.
\begin{equation}
    H(X)=-\sum_{i=1}^{n}P(x_{i})logP(x_{i})
\label{eq:entropy}
\end{equation}
The base of logarithm mentioned in the formula \ref{eq:entropy} can vary across different implementations. Base 2 is used
in this particular application as it was dictated by the method used in the paper \cite{entropy}.
The units of selected logarithm base are called bits. Naming of the units is strictly related to choice of the specific logarithm.
There are infinite number of possible bases but few of them are worth noticing. For instance, the base 10 can be useful in some
applications. The name of the units is ``dits'' in this particular example. Moreover, natural logarithm with the base of $e$
is also possible choice. In this case units are often referred as ``nats'' \cite{entropy_wiki}.
In this particular implementation, it assumed that $0*log_{2}0=0$ in the process of entropy calculation.

As it was suggested earlier, roots of entropy are present in the communication theory. This includes mainly work
provided by Claude Shannon. The initial assumption in this field is the definition of the data communication system.
Three components are included in this particular system. Data source is the first one, followed by communication channel
and lastly the receiver is specified. The main function of the latter is to recognize what was produced by the given source
relying only on the data propagated through the specific channel. Such functionality can be only established as long as
there are used some techniques to properly compress and decode the signal. Moreover, it is necessary to distinguish
the main signal from the noise. Multiple ways of encoding and compressing messages were discussed. The transmission
from the source was also taken into account in the Shannon's theory. It concludes to another definition of entropy,
the practical one. The entropy can be described as  ``an absolute mathematical limit on how well data from the source
can be losslessly compressed and sent through perfectly noiseless channel'' \cite{entropy_wiki}. 

The memoryless entropy was chosen as an estimator of real compression results. Although it is not ideal estimation,
such operation is good enough as results are proportional to these from JPEG 2000 coder \cite{entropy}.
Therefore entropy can be used just to determine choice between variants of DWT as described in \ref{sec:dwt_decompositions}.
However, such calculation of entropy can achieve not satisfying results for low filtered images.
The reason for such behavior lays in the very different characteristics of these images. The actual JPEG 2000 coder
is not context-free as memoryless entropy. High filtered images are free of such error due to context neutralizing
nature of such filters. To achieve same effect for low filtered images several methods were tested.
During tests it turned out that best results are achieved using difference between certain pixel and its left neighbor.

% TODO: write something about other methods

\section{Implementation details}

\subsection{Chosen programming language}

The features of given language exist to provide support for certain styles of programming. An individual
language feature should never be perceived as a solution. Bjarne Stroustrup in his book ``The C++ Programming Language''
suggests that language feature should be rather perceived as one building brick from a varied set which can
furtherly combined to express desired solutions \cite{cpp_language_bjarne}. Moreover, there are listed
basic requirements for design and programming:
\begin{itemize}
    \item Ideas shall directly be expressed in the code.
    \item Independent ideas shall be independently expressed in the code.
    \item Relationships shall be represented directly in code among other ideas.
    \item Expressed ideas shall be combined freely, where and only where combinations make sense.
    \item Simple ideas shall be expressed simply \cite{cpp_language_bjarne}.
\end{itemize}

Given that, the C++ programming language supports four styles of programming:
\begin{itemize}
    \item Procedural programming.
    \item Abstraction of data.
    \item Object-oriented programming.
    \item Generic programming.
\end{itemize}
Items in the list are not exclusive to each other. Moreover, as combination of chosen features
more desirable solution can be achieved. There are several aspects that make software application
successful, e.g. maintainability, readability, small size and fast time execution. To achieve it
for a nontrivial problem these styles are recommended to use in conjunction \cite{cpp_language_bjarne}.

The most notable solution present in the scope of algorithm implementation is generic programming.
This type of programming is focused on the design, implementation and runtime use of general purpose
algorithms. By ``general'' it is understood possibility of accepting variety of types by the algorithm.
However, these types have to meet specified requirements of the given algorithm. The main support
of generic programming in the C++ programming language is $template$ which provide static,
i.e. compile-time polymorphism \cite{cpp_language_bjarne}.

The C++11 standard introduced for the first time a thread library. It is heavily
influenced by its ancestors, e.g. Boost Thread Library. These Boost classes were the primary model
on which the standard library is based. Moreover, many entities share same names and structure \cite{cpp_concurrency}.
However, there are some disadvantages in the standard threading library. Notable features are
missing in comparison with other threading APIs, i.e. ``pthreads'' and Windows threads.
The reason lays in the extensive set of constraints on compiler implementers. Nevertheless,
multithreaded applications can be developed with standard behavior across all platforms.
This enables a solid foundation on which libraries utilizing parallelism can be built.
The concurrency elements of the Standard Library like futures, threads, mutexes and atomics
are the important but not the only available solutions for developing concurrent C++ software \cite{cpp_meyers}.

The thread-based programming provide higher level of abstraction in comparison to task-based one.
Therefore it makes thread management easier. However, there are multiple types of ``thread''
that can be encountered in concurrent C++ software:
\begin{itemize}
    \item Hardware threads are the ones that actually perform computation. Machine architectures
    offer one or more hardware threads per CPU core.
    \item Software threads, also known as system threads are the ones that operating system manages
    across all processes. The scheduler prepares their execution on hardware threads. Therefore, it
    is possible to create more software threads than the hardware ones. The motivation is execution
    of unblocked threads when the other ones are blocked, e.g. wait for mutex.
    \item ``std::threads'' are C++ objects that act like handles to underlying software threads.
    Standard thread can correspond to nothing so it represents ``null'' handle. There are several scenarios
    leading to this behavior. Default-constructed objects with no function to execute are the first
    example. The other ones include object state after being moved from, joined and detached thread \cite{cpp_meyers}.
\end{itemize}

\subsection{Build environment}

The build environment is setup around CMake. This software is open-sourced, cross-platform tool
designed to not only build but also test and package various applications. CMake is aimed to
control the compilation process of given software using compiler and platform agnostic configuration
text files. Such setup allows to generate platform native tools to build project, e.g. Makefiles
for GNU/Linux or nmake for Windows. Moreover, it is possible to generate Visual Studio project
files with the help of CMake. This software is also capable of generating wrappers, creating
both dynamic and static libraries and building executables in arbitrary combinations. Another
viable CMake feature is its caching system. Upon running build such text file is generated
and is ready to be investigated using graphical editor. The located files, libraries, executables
and optional build directives are placed in the cache \cite{cmake}.

The CMake tool is arranged to support complex structure of directory hierarchies and applications
which can be dependent on other libraries. The other complex scenario that can be handled by CMake
is building the executables in the specific order to generate code which is compiled and linked
into eventual application. The building processes is managed by CMakeLists.txt files that have
to be located in each directory containing source files. The sample CMakeLists.txt file can be
seen at the listing \ref{lst:sample_cmake_file}. This software provides not only predefined commands
but also a possibility of adding user functions.

\begin{listing}[!htb]
\begin{minted}[linenos, breaklines]{cmake}
project(jpeg2000_src)
add_subdirectory(dwt)
if(BUILD_EXTERNALS AND EXISTS ${JP3D_PATH})
    add_subdirectory(${JP3D_PATH})
endif()

set(SOURCES
    demo_dwt.cpp
    demo_queue.cpp
    demo_opencv.cpp
    arg_parser.cpp
    main.cpp
)
add_executable(jpeg2000 ${SOURCES})
set_target_properties(jpeg2000 PROPERTIES COMPILE_FLAGS "-save-temps")
if(BUILD_EXTERNALS)
    target_include_directories(jpeg2000 SYSTEM PRIVATE
        ${OPENCV_PATH}/modules/core/include
        ${OPENCV_PATH}/modules/imgcodecs/include
        ${OPENCV_PATH}/modules/imgproc/include
        ${CMAKE_BINARY_DIR} # OpenCV is looking for opencv2/opencv_modules.hpp
        ${CXXOPTS_PATH}/include
    )
    target_compile_definitions(jpeg2000 PRIVATE BUILD_OPENCV)
endif()
target_include_directories(jpeg2000 SYSTEM PRIVATE
    ${CMAKE_SOURCE_DIR}
    ${TIMER_PATH}
)
target_link_libraries(jpeg2000 PRIVATE
    dwt
    cxxopts
)
if(BUILD_EXTERNALS)
    target_link_libraries(jpeg2000 PRIVATE
        opencv_core
        opencv_imgcodecs
        opencv_imgproc
    )
    if(EXISTS ${JP3D_PATH})
        target_link_libraries(jpeg2000 PRIVATE jp3d)
    endif()
endif()
\end{minted}
\caption{Sample CMakeLists.txt file}
\label{lst:sample_cmake_file}
\end{listing}

\subsection{DWT interface}

As shown at the listing \ref{lst:dwt_interface}, two dimensional discrete wavelet transform
is strictly connected to the one dimensional. The reuse of this function makes design more
flexible. It is worth pointing out that these function are allocation agnostic. Therefore
the user is obligated allocate memory at first and then make use of dwt processing. Such
decision is dictated upon fact that actual type of matrix containing data is not specified.
Therefore a pointer to first element and total size are the most feasible solution.
There is one disadvantage that hits the runtime of presented algorithm. The results
from ``cols'' and ``rows'' functions are not cached in memory. Thus, upon calculating
``full'' DWT variant operations are unnecessarily doubled.

\begin{listing}[!htb]
\begin{minted}[linenos, breaklines]{cpp}
REGISTER_DWT_2D(dwt_2d_rows) {
    std::size_t out_cols = get_n_dwt_output(cols, n_filter);
    for (std::size_t i{}; i < rows; i++) {
        dwt_1d(input + i * cols,
               cols,
               filter,
               n_filter,
               output + i * out_cols,
               mode);
    }
}

REGISTER_DWT_2D(dwt_2d_cols) {
    for (std::size_t i{}; i < cols; i++) {
        dwt_1d(input + i, rows, filter, n_filter, output + i, mode, cols);
    }
}

REGISTER_DWT_2D(dwt_2d) {
    std::size_t out_cols = get_n_dwt_output(cols, n_filter);
    std::vector<T> tmp;
    tmp.reserve(out_cols * rows);
    dwt_2d_rows(input, rows, cols, filter, n_filter, tmp.data(), mode);
    dwt_2d_cols(tmp.data(), rows, out_cols, filter, n_filter, output, mode);
}
\end{minted}
\caption{Two dimensional discrete wavelet implementation}
\label{lst:dwt_interface}
\end{listing}

\subsection{Testing}

The testing, especially test driven development takes essential place in the software development
process. For the purpose of checking correctness of proposed discrete wavelet transform algorithm
``PyWavelets'' (Wavelet Transforms in Python) software was used as the reference model \cite{pywavelets}.
Thanks to pybind11 \cite{pybind11} C++ was exposed to the Python and then tested using pytest framework.
The first one is a lightweight header-only library that synergizes both Python and C++. The main purpose
is to create Python bindings for existing C++ code. Boilerplate code is minimized thanks to inferring
type information using introspection during compile-time \cite{pywavelets}. On the other hand pytest framework makes
writing small and readable tests easier. Moreover, it can scale up to support complex functional
testing for other libraries and applications \cite{pytest}.

The used technique to check correctness of written DWT functions in this application are testing
fixtures. Their purpose is to provide a defined, reliable and consistent context for the written tests.
The environment, e.g. database configured with known parameters or content, e.g. dataset can also
be included in such a fixture. Moreover, steps and data constituting the arrange phase of a test
are defined. For more complex scenarios fixtures can be also used to define active phase 
of given test which is a case in the thesis. Moreover, the state, services and other operating
environment set up by the fixtures can be accessed by testing functions through argument lists.
Every fixture that is used by test function has its special parameter named after the fixture name
in the definition of the function \cite{pytest}. The example fixture defined with the help
of pytest is present at the listing \ref{lst:dwt_test_fixture}.

\begin{listing}[!htb]
\begin{minted}[linenos, breaklines]{python}
def get_all_test_suits():
    return chain(
        get_precision_test_suits(np.float32),
        get_precision_test_suits(np.float64)
    )


@pytest.fixture
def dwt_fixture(request):
    data, wavelet, padding_mode, dwt_func = request.param
    precision = data.dtype.name
    dwt_ref, dwt_impl = DWT_FUNCS[dwt_func]
    dwt_impl = dwt_impl[PRECISION[precision]]
    ref = dwt_ref(data, wavelet, padding_mode)
    out = dwt_impl(data, WAVELETS[wavelet][PRECISION[precision]],
                    PADDING_MODES[padding_mode])
    print(f"{ref}\n{out}")
    return ref, out


@pytest.mark.parametrize("dwt_fixture", get_all_test_suits(), indirect=True)
def test_dwt(dwt_fixture):
    assert np.allclose(*dwt_fixture)
\end{minted}
\caption{Test fixture of DWT}
\label{lst:dwt_test_fixture}
\end{listing}

\subsection{Parallel for} \label{sec:parallel_for}

The base function of the parallelized for version is available at the listing \ref{lst:parallel_for_base_function}.
The signature of this function makes its usage very flexible. All types of callable entities
can be employed to perform operations on specified thread index. Moreover, vector from Standard
Template Library is used as safe and extensible abstraction over dynamically created arrays.
Each thread from c++ standard requires initialization with some type of callback to
be able to start. The thing that is worth to notice is intentional pass of such function by
constant reference instead of forwarding reference. The reason of this implementation is that
threads should operate on copied context from the main thread. There is no advantage of moving
function objects between the master thread and first forked one because other threads may
end up with hanging state of its context. Lastly, main thread has to wait for worker threads
to be ready with their specified tasks. Therefore join method is used at the end of scope.

\begin{listing}[!htb]
\begin{minted}[linenos, breaklines]{cpp}
#ifndef JPEG2000_PARALLEL_FOR_HPP
#define JPEG2000_PARALLEL_FOR_HPP

#include "config.hpp"

#include <concepts>
#include <functional>
#include <thread>
#include <vector>

namespace mgr {
namespace detail {

// clang-format off
template<typename Func, typename... Args>
// std::invocable<Func, Args...> should really be used in conjunction
// enable it with the release of libc++13 (support of <concepts> header)
concept no_returnable = std::same_as<std::invoke_result_t<Func, Args...>, void>;

template<typename Func, typename... Args>
concept returnable = !no_returnable<Func, Args...>;
// clang-format on

template<typename Func>
void parallel_for(std::size_t n_threads, const Func& func) {
    std::vector<std::thread> threads;
    threads.reserve(n_threads);
    for (std::size_t thread_idx{}; thread_idx < n_threads; thread_idx++) {
        threads.emplace_back(func, thread_idx);
    }
    for (auto& thread : threads) {
        thread.join();
    }
}
} // namespace detail
\end{minted}
\caption{parallel\_for.hpp: Base function}
\label{lst:parallel_for_base_function}
\end{listing}

User interface of the parallel for is available at the listing \ref{lst:parallel_for_user_interface}.
Two separate functions were developed during implementation phase of thesis creation.
Thanks to one of the major features of C++20, i.e. concepts, interface of these functions
is not that complicated. Other solutions include using SFINAE based approach which is very
hard to reason about and has many pitfalls. Another possible substitute of concepts is just
different naming of these functions. The drawback of this scenario is loose in readability
and usability. It is worth noticing that there is no possibility of overload function basing
on its return type in C++. The reason of this behavior is that return type is not included
in the signature of function.

The first function allows to conveniently parallelize set of computations with no return type.
Thanks to functionalities split there is no runtime penalty in this case. From the user perspective
any callable type can be passed as parameter of this function. However, there is one unnamed
requirement. Fortunately enough it is caught during compilation time instead of runtime. Passed
function-like object has to be invoked with one argument. The reason of such design is to
make user responsible for specifying what should happen during calculation process of each
loop iteration. During dispatchment of first for iteration each thread starts at the different index.
Furtherly such index is incremented with number of threads specified by the user. Therefore,
each calculation is guaranteed to take place in the appropriate thread context.

The second function extends the properties of the first one by adding the possibility of returning
value from each loop iteration. The return type is deduced thanks to compile time introspection
capabilities of ``std::invoke\_result'' which works on any callable entity \cite{cppreference}. 
It is worth to notice that ``std::vector'' which stores eventual results is zero-initialized
at first. The reasoning includes concurrent access to its elements. Therefore, it is virtually
impossible to ensure that ``push\_back'' or ``emplace\_back'' methods do not result in data race
without locking mechanism.

\begin{listing}[!htb]
\begin{minted}[linenos, breaklines]{cpp}
template<typename Func>
requires detail::no_returnable<Func, std::size_t>
void parallel_for(std::size_t n_threads, std::size_t n_elements, Func&& func) {
    detail::parallel_for(
        n_threads,
        [n_threads, n_elements, func = std::forward<Func>(func)](
            std::size_t thread_idx) mutable {
            for (std::size_t i{thread_idx}; i < n_elements; i += n_threads) {
                func(i);
            }
        });
}

template<typename Func>
requires detail::returnable<Func, std::size_t>
auto parallel_for(std::size_t n_threads, std::size_t n_elements, Func&& func) {
    std::vector<std::invoke_result_t<Func, std::size_t>> result(n_elements);
    detail::parallel_for(
        n_threads,
        [n_threads, n_elements, func = std::forward<Func>(func), &result](
            std::size_t thread_idx) mutable {
            for (std::size_t i{thread_idx}; i < n_elements; i += n_threads) {
                result[i] = func(i);
            }
        });
    return result;
}
} // namespace mgr

#endif // JPEG2000_PARALLEL_FOR_HPP
\end{minted}
\caption{parallel\_for.hpp: User interface}
\label{lst:parallel_for_user_interface}
\end{listing}

\subsection{Queue generation}

The calculation process of DWT processing size is available at the listing \ref{lst:dwt_queue}.
As was described earlier in the thesis, depth of DWT calculations is set to five. From the
mathematical point of view, such processing queue is a cartesian product of its values.
However, the processing chain includes not performing discrete wavelet transform at all
and returning early from algorithm. Therefore, it was decided to limit scope of possible
results. This function features new keyword in C++20, i.e. ``consteval'' which makes sure
that function can be used only in the compile time.

\begin{listing}[!htb]
\begin{minted}[linenos, breaklines]{cpp}
#ifndef JPEG2000_QUEUE_HPP
#define JPEG2000_QUEUE_HPP

#include "config.hpp"
#include "dwt_2d.hpp"
#include "template_helpers.hpp"

#include <array>

namespace mgr {
namespace detail {

constexpr std::size_t depth = 5;
constexpr std::size_t n_dwt_cbs = lut_dwt_2d_cbs<float, float>.size() - 1;

consteval std::size_t get_queue_size() {
    std::size_t temp{1};
    std::size_t result{temp};
    for (std::size_t i{}; i < depth; i++) {
        temp *= n_dwt_cbs;
        result += temp;
    }
    return result;
}
\end{minted}
\caption{Calculation of size in DWT processing queue}
\label{lst:dwt_queue_size}
\end{listing}

The generation and then instantiation of DWT processing queue is available at the consecutive listings
\ref{lst:dwt_queue} and \ref{lst:dwt_queue_instantiation}. As it was specified earlier, these
computations are done in the compile time thanks to ``template'' and ``constexpr'' meta programming.
The instantiation process requires three pieces of information. First one is type of single
element in the queue which is set up to specific DWT callback in this case. Second one is related
to the element which is supposed to stop chain of calculations. Lastly, array containing all
needed callbacks is required.

On the other hand, generation process is more convoluted. The main algorithm consists of recursive
call of function which performs calculation of modified cartesian product described earlier.
In the basic path previously generated arguments are forwarded and the new one is appended
at the end. However, if the ``stop'' operation is detected, the recursive chain is terminated
and the results are saved to the buffer. Last case involves break of the whole process when
the maximum DWT level is reached. All of these steps are performed during compile-time so
there is no runtime penalty. In the result look-up table is produced in the ``.rodata''
memory section.

\begin{listing}[!htb]
\begin{minted}[linenos, breaklines]{cpp}
template<typename Elem, Elem Pad>
struct cartesian_prod {
    template<typename T,
                std::size_t CartesianCounter = depth,
                typename... Args>
    consteval auto calculate(T&& array, Args&&... args) {
        for (std::size_t i{}; i < array.size(); i++) {
            if constexpr (CartesianCounter != 0) {
                if (i) {
                    calculate<T, CartesianCounter - 1>(
                        array,
                        std::forward<Args>(args)...,
                        array[i]);
                } else {
                    fill_queue(pad_sequence<Elem, Pad, CartesianCounter>{},
                                std::forward<Args>(args)...);
                }
            } else {
                fill_queue(std::forward<Args>(args)...);
                break;
            }
        }
        return queue_;
    }

private:
    template<typename... Args, typename T, T... Is>
    consteval void fill_queue(sequence<T, Is...>, Args&&... args) {
        queue_[queue_counter_++] = std::to_array(
            {std::forward<Args>(args)..., Is...});
    }

    template<typename... Args>
    consteval void fill_queue(Args&&... args) {
        queue_[queue_counter_++] = std::to_array(
            {std::forward<Args>(args)...});
    }

    std::array<std::array<Elem, depth>, get_queue_size()> queue_{};
    std::size_t queue_counter_{};
};
} // namespace detail
\end{minted}
\caption{Generation of DWT processing queue}
\label{lst:dwt_queue}
\end{listing}

\begin{listing}[!htb]
\begin{minted}[linenos, breaklines]{cpp}
template<typename T>
inline constexpr auto
    dwt_queue = detail::cartesian_prod<dwt_2d_cb<T, T>, no_dwt_2d<T, T>>{}
                    .calculate(lut_dwt_2d_cbs<T, T>);

} // namespace mgr
\end{minted}
\caption{Instantiation of DWT processing queue}
\label{lst:dwt_queue_instantiation}
\end{listing}

\subsection{OpenCV}

The OpenCV, i.e. Open Source Computer Vision Library, is a software module that includes
several hundreds of specific algorithms used in computer vision. This library has modular
structure. Therefore, packages can include several shared or static sub-libraries.
Since version 2.x OpenCV exposes only C++ API in opposition to C based first version.
The following list of modules is available:
\begin{itemize}
    \item Core functionality is a compact library that defines basic data structures. Dense
    multi-dimensional array, i.e. $Mat$ and fundamental functions, used all over the place
    by other parts of the OpenCV, are included in this module.
    \item Image Processing includes both liner and non-linear image filtering and geometrical
    transformations such as resizing, affine and perspective warping. Moreover, this module
    support color space conversion and histogram calculation. 
    \item Video Analysis consists of features like motion estimation, background
    subtraction and object tracking algorithms.
    \item Camera Calibration and 3D Reconstruction includes basic multiple-view geometry
    algorithms, both single and stereo camera calibration, object pose estimation, stereo
    correspondence algorithms and elements of three dimensional reconstruction.
    \item 2D Features Framework consists of salient feature detectors, descriptors and their matchers. 
    \item Object Detection features possibility of objects and instances of predefined classes detection.
    It includes for instance faces, eyes, mugs, people and cars. 
    \item High-Level GUI is a simple interface supporting basic user needs.
    \item Video I/O supports video capturing of various video codecs.
    \item Other helper modules, e.g. Google test wrapper and Python bindings.
\end{itemize}
